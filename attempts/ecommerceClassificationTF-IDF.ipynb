{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация товаров\n",
    "Основная идея данной части проекта - создание модели, классифицирующей текст-описание товаров в 4 категории (Electronics, Household, Books, and Clothing & Accessories)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начало\n",
    "Подключим необходимые библиотеки, обьявим необходимые функции и константы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import string, nltk\n",
    "from string import punctuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = TfidfVectorizer(max_features= 12000, ngram_range = (1, 1))\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление текста\n",
    "Модели работают только с числовым представлением текстов, для чего необходимо строить их вектора, а для того, чтобы вектора были построены наиболее эффективным способом, нам надо очистить текст от \"мусора\", а также привести его в унифицированный вид.\n",
    "Ниже представлены функции для обработки текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def text_remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "def text_stem(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in tokenizer.tokenize(text)])\n",
    "\n",
    "def text_lemm(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokenizer.tokenize(text)])\n",
    "\n",
    "def text_remove_stopwords(text):\n",
    "    stoplist = stopwords.words('english')\n",
    "    text_no_stopwords = [word for word in tokenizer.tokenize(text) if word not in stoplist]\n",
    "    return \" \".join(text_no_stopwords)\n",
    "\n",
    "def text_remove_punct(text):\n",
    "    text_no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text_no_punct\n",
    "\n",
    "\n",
    "def text_remove_nonalph(text):\n",
    "    return \" \".join([word for word in tokenizer.tokenize(text) if word.isalpha()])\n",
    "\n",
    "def text_required_pos(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    taglist = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    text_onlypos = [x[0] for x in tokens_tagged if x[1] in taglist]\n",
    "    return \" \".join(text_onlypos)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложим все эти функции в одну, ее то мы и будем использовать для обработки каждого описания товара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = text_lower(text)\n",
    "    text = text_remove_whitespaces(text)\n",
    "    text = text_remove_punct(text)\n",
    "    text = text_remove_stopwords(text)\n",
    "    #text = text_stem(text)\n",
    "    text = text_lemm(text)\n",
    "    text = text_remove_nonalph(text)\n",
    "    text = text_required_pos(text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "В задаче данные подают в виде csv файла, нам нужно из него получить датасет для обучения и тестирования.\n",
    "Ниже представлена функция открывающая конвертирующая csv файл в датафрейм из pandas, далее его мы уже будем представлять в виде датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.columns = ['category', 'description']\n",
    "    category_dict = {'Household': 0, 'Books': 1, 'Clothing & Accessories': 2, 'Electronics' : 3}\n",
    "    data.replace({'category' : category_dict}, inplace=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data('ecommerceDataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем описания товаров, сохраним отдельно входы (описание товара) и выходы (категория товара)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(data['description'].apply(text_preprocess))\n",
    "y = list(data['category'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем наши данные на тренировочные и тестовые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data)\n",
    "indexs = [idx for idx in range(data_size)]\n",
    "train_indices, test_indices = [], []\n",
    "np.random.shuffle(indexs)\n",
    "train_size = int(data_size * 0.8)\n",
    "for i in range(data_size):\n",
    "    if i < train_size:\n",
    "        train_indices.append(indexs[i])\n",
    "    else:\n",
    "        test_indices.append(indexs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x[idx] for idx in train_indices]\n",
    "y_train = [y[idx] for idx in train_indices]\n",
    "x_test = [x[idx] for idx in test_indices]\n",
    "y_test = [y[idx] for idx in test_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, конвертируем обработанные текст-данные в вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_x_train = vectorizer.fit_transform(x_train)\n",
    "vect_x_test = vectorizer.transform(x_test)\n",
    "vect_x_train = vect_x_train.toarray()\n",
    "vect_x_test = vect_x_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlen = len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcommerceDataset(Dataset):\n",
    "    def __init__(self, x_vectorized, y):\n",
    "        self.x_vectorized = x_vectorized\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_vectorized[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EcommerceDataset(vect_x_train, y_train)\n",
    "test_dataset = EcommerceDataset(vect_x_test, y_test)\n",
    "trainval_size = len(train_dataset)\n",
    "val_size = int(trainval_size * 0.15)\n",
    "train_size = trainval_size - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель\n",
    "Для модели классификации текста за основу выберем DenseNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyDenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputlen,1024)\n",
    "        self.fc2 = nn.Linear(1024,256)\n",
    "        self.prediction = nn.Linear(256,4)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.to(torch.float)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.prediction(x),dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyDenseNet().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели и тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию тестирования и обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, dataloader, loss_fn):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    num_correct = 0\n",
    "    num_elements = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            losses.append(loss.item())\n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y_batch).item()\n",
    "            num_elements += y_batch.size(0)\n",
    "\n",
    "    accuracy = round(100 * num_correct / num_elements, 2)\n",
    "    return accuracy, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, loss_fn, optimizer, n_epoch=6):\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        #epoch_loss = []\n",
    "        #epoch_true = 0\n",
    "        #epoch_total = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(x_batch)\n",
    "            \n",
    "            loss = loss_fn(outputs,y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            #epoch_loss.append(loss.item())\n",
    "            #epoch_true += torch.sum(y_pred == y_batch).item()\n",
    "            #epoch_total += y_batch.size(0)\n",
    "            #y_pred = torch.argmax(outputs,dim=1)\n",
    "        \n",
    "        #epoch_accuracy = round(100 * epoch_true / epoch_total, 2)\n",
    "        epoch_accuracy, epoch_loss = evaluate(model, device, val_loader, loss_fn)\n",
    "        print(f\"Epoch {epoch}/{n_epoch} finished: train_accuracy = {epoch_accuracy}%, train_loss = {np.mean(epoch_loss)}\")\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 finished: train_accuracy = 93.74%, train_loss = 0.2726059578499704\n",
      "Epoch 2/6 finished: train_accuracy = 94.63%, train_loss = 0.19439372799868854\n",
      "Epoch 3/6 finished: train_accuracy = 95.14%, train_loss = 0.18123840393041665\n",
      "Epoch 4/6 finished: train_accuracy = 95.38%, train_loss = 0.17818583850309533\n",
      "Epoch 5/6 finished: train_accuracy = 95.41%, train_loss = 0.18597044366991744\n",
      "Epoch 6/6 finished: train_accuracy = 95.17%, train_loss = 0.1945536150333454\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, device, criterion, optimizer, n_epoch=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing finished: Accuracy =  95.29%, Loss = 0.20141273951050878\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_loss = evaluate(model, device, test_loader, criterion)\n",
    "print(f'Testing finished: Accuracy =  {test_accuracy}%, Loss = {test_loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "Мы получили оригинальную модель, которая работает хорошо, если к нам добавляют только продукты из описанных выше категорий :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'Household': 0, 'Books': 1, 'Clothing & Accessories': 2, 'Electronics' : 3}\n",
    "def predict_single(text):\n",
    "    text = text_preprocess(text)\n",
    "    x = vectorizer.transform([text])\n",
    "    x = torch.sparse_coo_tensor(x.nonzero(), x.data, x.shape)\n",
    "    pred = model(x)\n",
    "    pred = pred.argmax(axis=1)\n",
    "    return list(mydict.keys())[list(mydict.values()).index(pred[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data's category based on desription is: Household (Paper Plane Design Framed Wall Hanging ...) \n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction data\\'s category based on desription is: {predict_single(data[\"description\"][0])} ({data[\"description\"][0][:39]}...) ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
